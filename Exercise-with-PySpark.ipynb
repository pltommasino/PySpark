{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "spark = SparkSession.builder.appName('Name').config('config_option', 'value').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f619e91-96b4-4f70-975c-75024aa07cba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product_id                                       product_name aisle_id  \\\n",
      "0          1                         Chocolate Sandwich Cookies       61   \n",
      "1          2                                   All-Seasons Salt      104   \n",
      "2          3               Robust Golden Unsweetened Oolong Tea       94   \n",
      "3          4  Smart Ones Classic Favorites Mini Rigatoni Wit...       38   \n",
      "4          5                          Green Chile Anytime Sauce        5   \n",
      "5          6                                       Dry Nose Oil       11   \n",
      "6          7                     Pure Coconut Water With Orange       98   \n",
      "7          8                  Cut Russet Potatoes Steam N' Mash      116   \n",
      "8          9                  Light Strawberry Blueberry Yogurt      120   \n",
      "9         10     Sparkling Orange Juice & Prickly Pear Beverage      115   \n",
      "\n",
      "  department_id  \n",
      "0            19  \n",
      "1            13  \n",
      "2             7  \n",
      "3             1  \n",
      "4            13  \n",
      "5            11  \n",
      "6             7  \n",
      "7             1  \n",
      "8            16  \n",
      "9             7  \n",
      "The row of products.csv are: 49688\n"
     ]
    }
   ],
   "source": [
    "#DATAFRAME  PRODUCTS\n",
    "products = spark.read.csv(\"/Users/pasquale/Desktop/MySQL/EsameBDBD2020_CarvigianiDelleFrattePicconiTommasino/archive/products.csv\", header=\"true\")\n",
    "#products.cache() # Cache data for faster reuse\n",
    "products = products.dropna() # drop rows with missing values\n",
    "#products.show(10)\n",
    "products_df = products.toPandas()\n",
    "print(products_df.head(10))\n",
    "#products.limit(10).toPandas()\n",
    "a=products.count()\n",
    "print(\"The row of products.csv are: %s\" %a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ba795a7-92ff-4ccb-86f0-b1f6ae96cdd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 17:41:20 WARN CacheManager: Asked to cache already cached data.\n",
      "+-------------+---------------+\n",
      "|department_id|     department|\n",
      "+-------------+---------------+\n",
      "|            1|         frozen|\n",
      "|            2|          other|\n",
      "|            3|         bakery|\n",
      "|            4|        produce|\n",
      "|            5|        alcohol|\n",
      "|            6|  international|\n",
      "|            7|      beverages|\n",
      "|            8|           pets|\n",
      "|            9|dry goods pasta|\n",
      "|           10|           bulk|\n",
      "+-------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "The row of departments.csv are: 21\n"
     ]
    }
   ],
   "source": [
    "#DATAFRAME DEPARTMENTS\n",
    "departments = spark.read.csv(\"/Users/pasquale/Desktop/MySQL/EsameBDBD2020_CarvigianiDelleFrattePicconiTommasino/archive/departments.csv\", header=\"true\", inferSchema=\"true\")\n",
    "departments.cache() # Cache data for faster reuse\n",
    "departments = departments.dropna() # drop rows with missing values\n",
    "departments.show(10)\n",
    "b = departments.count()\n",
    "print(\"The row of departments.csv are: %s\" %b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9e602250-4256-4d4b-b201-843f0e8b2bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: int, user_id: int, eval_set: string, order_number: int, order_dow: int, order_hour_of_day: int, days_since_prior_order: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The row of orders.csv are: 3214874\n"
     ]
    }
   ],
   "source": [
    "#DATAFRAME ORDERS\n",
    "orders = spark.read.csv(\"/Users/pasquale/Desktop/MySQL/EsameBDBD2020_CarvigianiDelleFrattePicconiTommasino/archive/orders.csv\", header=\"true\", inferSchema=\"true\")\n",
    "#orders.cache() # Cache data for faster reuse\n",
    "orders = orders.dropna() # drop rows with missing values\n",
    "orders.take(10)\n",
    "display(orders)\n",
    "\n",
    "d = orders.count()\n",
    "print(\"The row of orders.csv are: %s\" %d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "22247f99-b064-4081-8d26-d2a969a61196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 17:41:22 WARN CacheManager: Asked to cache already cached data.\n",
      "+--------+--------------------+\n",
      "|aisle_id|               aisle|\n",
      "+--------+--------------------+\n",
      "|       1|prepared soups sa...|\n",
      "|       2|   specialty cheeses|\n",
      "|       3| energy granola bars|\n",
      "|       4|       instant foods|\n",
      "|       5|marinades meat pr...|\n",
      "|       6|               other|\n",
      "|       7|       packaged meat|\n",
      "|       8|     bakery desserts|\n",
      "|       9|         pasta sauce|\n",
      "|      10|    kitchen supplies|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "The row of aisles.csv are: 134\n"
     ]
    }
   ],
   "source": [
    "#DATAFRAME AISLES\n",
    "aisles = spark.read.csv(\"/Users/pasquale/Desktop/MySQL/EsameBDBD2020_CarvigianiDelleFrattePicconiTommasino/archive/aisles.csv\", header=\"true\", inferSchema=\"true\")\n",
    "aisles.cache() # Cache data for faster reuse\n",
    "aisles = aisles.dropna() # drop rows with missing values\n",
    "aisles.show(10)\n",
    "c = aisles.count()\n",
    "print(\"The row of aisles.csv are: %s\" %c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3bf0dbf0-6731-4790-959b-a51404f366a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "|         3|Robust Golden Uns...|      94|            7|\n",
      "|         4|Smart Ones Classi...|      38|            1|\n",
      "|         5|Green Chile Anyti...|       5|           13|\n",
      "|         6|        Dry Nose Oil|      11|           11|\n",
      "|         7|Pure Coconut Wate...|      98|            7|\n",
      "|         8|Cut Russet Potato...|     116|            1|\n",
      "|         9|Light Strawberry ...|     120|           16|\n",
      "|        10|Sparkling Orange ...|     115|            7|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE TEMPORARY TABLE OF PRODUCTS\n",
    "products.createOrReplaceTempView(\"productsstudy\")\n",
    "sqldf1 = spark.sql(\"SELECT * FROM productsstudy\")\n",
    "sqldf1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eebb9d8a-486c-4c1b-b7f6-14c4481e46df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|department_id|     department|\n",
      "+-------------+---------------+\n",
      "|            1|         frozen|\n",
      "|            2|          other|\n",
      "|            3|         bakery|\n",
      "|            4|        produce|\n",
      "|            5|        alcohol|\n",
      "|            6|  international|\n",
      "|            7|      beverages|\n",
      "|            8|           pets|\n",
      "|            9|dry goods pasta|\n",
      "|           10|           bulk|\n",
      "+-------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE TEMPORARY TABLE OF DEPARTMENTS\n",
    "departments.createOrReplaceTempView(\"departmentsstudy\")\n",
    "sqldf2 = spark.sql(\"SELECT * FROM departmentsstudy\")\n",
    "sqldf2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8f3ca5c-98f1-4332-9598-f62196282c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|aisle_id|               aisle|\n",
      "+--------+--------------------+\n",
      "|       1|prepared soups sa...|\n",
      "|       2|   specialty cheeses|\n",
      "|       3| energy granola bars|\n",
      "|       4|       instant foods|\n",
      "|       5|marinades meat pr...|\n",
      "|       6|               other|\n",
      "|       7|       packaged meat|\n",
      "|       8|     bakery desserts|\n",
      "|       9|         pasta sauce|\n",
      "|      10|    kitchen supplies|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE TEMPORARY TABLE OF AISLES\n",
    "aisles.createOrReplaceTempView(\"aislesstudy\")\n",
    "sqldf3 = spark.sql(\"SELECT * FROM aislesstudy\")\n",
    "sqldf3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "346da979-4728-4663-8091-8c9edc2aaaf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|        3|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|        4|               15|                  28.0|\n",
      "| 3367565|      1|   prior|           6|        2|                7|                  19.0|\n",
      "|  550135|      1|   prior|           7|        1|                9|                  20.0|\n",
      "| 3108588|      1|   prior|           8|        1|               14|                  14.0|\n",
      "| 2295261|      1|   prior|           9|        1|               16|                   0.0|\n",
      "| 2550362|      1|   prior|          10|        4|                8|                  30.0|\n",
      "| 1187899|      1|   train|          11|        4|                8|                  14.0|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE TEMPORARY TABLE OF ORDERS\n",
    "orders.createOrReplaceTempView(\"ordersstudy\")\n",
    "sqldf4 = spark.sql(\"SELECT * FROM ordersstudy\")\n",
    "sqldf4.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a622bb13-ead3-432d-88d2-7a09d7e2f59d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RENAME COLUMN \"DAY_OF_WEEK\" AND RENAME DF IN \"ORDERS_DAY\"\n",
    "ORDERS_DAY = orders.withColumnRenamed(\"order_dow\", \"day_of_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffa6b9f2-06b4-4b85-8c37-a226610d4a9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(day_of_week)|\n",
      "+----------------+\n",
      "|               6|\n",
      "+----------------+\n",
      "\n",
      "+----------------+\n",
      "|min(day_of_week)|\n",
      "+----------------+\n",
      "|               0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FIND THE CORRECT VARIABLES\n",
    "from pyspark.sql import functions\n",
    "ORDERS_DAY.groupBy().max(\"day_of_week\").show()\n",
    "ORDERS_DAY.groupBy().min(\"day_of_week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1be1b63-0332-4c15-9a09-43c0d9c1278a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------------+-----------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|day_of_week|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+-----------+-----------------+----------------------+\n",
      "| 2398795|      1|   prior|           2|  Mercoledì|                7|                  15.0|\n",
      "|  473747|      1|   prior|           3|  Mercoledì|               12|                  21.0|\n",
      "| 2254736|      1|   prior|           4|    Giovedì|                7|                  29.0|\n",
      "|  431534|      1|   prior|           5|    Giovedì|               15|                  28.0|\n",
      "| 3367565|      1|   prior|           6|    Martedì|                7|                  19.0|\n",
      "|  550135|      1|   prior|           7|     Lunedì|                9|                  20.0|\n",
      "| 3108588|      1|   prior|           8|     Lunedì|               14|                  14.0|\n",
      "| 2295261|      1|   prior|           9|     Lunedì|               16|                   0.0|\n",
      "| 2550362|      1|   prior|          10|    Giovedì|                8|                  30.0|\n",
      "| 1187899|      1|   train|          11|    Giovedì|                8|                  14.0|\n",
      "+--------+-------+--------+------------+-----------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RENAME DAY OF WEEK\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '0', 'Domenica'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '1', 'Lunedì'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '2', 'Martedì'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '3', 'Mercoledì'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '4', 'Giovedì'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '5', 'Venerdì'))\n",
    "ORDERS_DAY = ORDERS_DAY.withColumn('day_of_week', regexp_replace('day_of_week', '6', 'Sabato'))\n",
    "                                   \n",
    "ORDERS_DAY.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a49295a-c433-4ab4-9022-ded293b25bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sum_order|\n",
      "+---------+\n",
      "| 58481984|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE TABLE OF SUM OF ORDERS\n",
    "sqldf5 = spark.sql(\"SELECT SUM(order_number) as sum_order FROM ordersstudy\")\n",
    "sqldf5.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d07988a1-1f0d-454e-bb8c-8262b2740da8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(order_id)=5499605950865, sum(user_id)=331035818017, sum(order_number)=58481984, sum(order_hour_of_day)=43210534, sum(days_since_prior_order)=35732798.0)]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORDERS_DAY.groupBy().sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "620ff30e-0ac3-41cf-87b3-7410efdbf8b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 698:=====>                                                  (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|day_of_week|total_orders|\n",
      "+-----------+------------+\n",
      "|     Lunedì|    10077792|\n",
      "|   Domenica|     9523062|\n",
      "|    Martedì|     8108244|\n",
      "|    Venerdì|     8082294|\n",
      "|  Mercoledì|     7761683|\n",
      "|    Giovedì|     7653131|\n",
      "|     Sabato|     7275778|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#TABLE OF TOTAL ORDERS PER DAY OF WEEK (DECREASING WAY)\n",
    "ORDERS_DAY.createOrReplaceTempView(\"total_orders\")\n",
    "sqldf6 = spark.sql(\" select day_of_week, sum(order_number) as total_orders from total_orders group by 1 order by 2 desc limit 7 \")\n",
    "sqldf6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39f0c44b-bd26-4c16-bfb3-28f1393456c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 701:=====>                                                  (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|day_of_week|total_orders|\n",
      "+-----------+------------+\n",
      "|     Lunedì|    10077792|\n",
      "|   Domenica|     9523062|\n",
      "|    Martedì|     8108244|\n",
      "|    Venerdì|     8082294|\n",
      "|  Mercoledì|     7761683|\n",
      "|    Giovedì|     7653131|\n",
      "|     Sabato|     7275778|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#BAR CHART OF TOTAL ORDERS PER DAY OF WEEK (DECREASING WAY)\n",
    "ORDERS_DAY.createOrReplaceTempView(\"total_orders\")\n",
    "sqldf7= spark.sql(\"\"\"\n",
    "  select day_of_week, sum(order_number) as total_orders\n",
    "  from total_orders\n",
    "  group by 1\n",
    "  order by 2 desc\n",
    "  limit 7\n",
    "\"\"\")\n",
    "sqldf7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ee648b3-fdab-4ebd-8f1e-c0016a2d2ead",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(order_hour_of_day)|\n",
      "+----------------------+\n",
      "|                    23|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|min(order_hour_of_day)|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FIND THE CORRECT VARIABLES\n",
    "from pyspark.sql import functions\n",
    "ORDERS_DAY.groupBy().max(\"order_hour_of_day\").show()\n",
    "ORDERS_DAY.groupBy().min(\"order_hour_of_day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b13420a-d23f-431b-9163-d6271c799d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_hour_of_day: int, total_orders: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TABLE OF TOTAL ORDERS PER HOUR\n",
    "ORDERS_DAY.createOrReplaceTempView(\"order_hour\")\n",
    "display(spark.sql(\"\"\"\n",
    "  select order_hour_of_day, sum(order_number) as total_orders\n",
    "  from order_hour\n",
    "  group by 1 \n",
    "  order by 1 asc\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1eeca507-d200-4769-bc7a-30f498b29155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_hour_of_day: int, total_orders: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TREND LINE OF TOTAL ORDERS PER HOUR\n",
    "ORDERS_DAY.createOrReplaceTempView(\"order_hour\")\n",
    "display(spark.sql(\"\"\"\n",
    "  select order_hour_of_day, sum(order_number) as total_orders\n",
    "  from order_hour\n",
    "  group by 1 \n",
    "  order by 1 asc\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5eaa26f-e885-4f91-8251-de780deb61ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+--------+---------------+\n",
      "|department_id|product_id|        product_name|aisle_id|     department|\n",
      "+-------------+----------+--------------------+--------+---------------+\n",
      "|           19|         1|Chocolate Sandwic...|      61|         snacks|\n",
      "|           13|         2|    All-Seasons Salt|     104|         pantry|\n",
      "|            7|         3|Robust Golden Uns...|      94|      beverages|\n",
      "|            1|         4|Smart Ones Classi...|      38|         frozen|\n",
      "|           13|         5|Green Chile Anyti...|       5|         pantry|\n",
      "|           11|         6|        Dry Nose Oil|      11|  personal care|\n",
      "|            7|         7|Pure Coconut Wate...|      98|      beverages|\n",
      "|            1|         8|Cut Russet Potato...|     116|         frozen|\n",
      "|           16|         9|Light Strawberry ...|     120|     dairy eggs|\n",
      "|            7|        10|Sparkling Orange ...|     115|      beverages|\n",
      "|            7|        11|   Peach Mango Juice|      31|      beverages|\n",
      "|            1|        12|Chocolate Fudge L...|     119|         frozen|\n",
      "|           11|        13|   Saline Nasal Mist|      11|  personal care|\n",
      "|           17|        14|Fresh Scent Dishw...|      74|      household|\n",
      "|           18|        15|Overnight Diapers...|      56|         babies|\n",
      "|           19|        16|Mint Chocolate Fl...|     103|         snacks|\n",
      "|           12|        17|   Rendered Duck Fat|      35|   meat seafood|\n",
      "|            1|        18|Pizza for One Sup...|      79|         frozen|\n",
      "|            9|        19|Gluten Free Quino...|      63|dry goods pasta|\n",
      "|            7|        20|Pomegranate Cranb...|      98|      beverages|\n",
      "+-------------+----------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MERGE DEPARTMENTS AND PRODUCTS WITH \"LEFT OUTER JOIN\"\n",
    "left_join = products.join(departments, on=\"department_id\", how='left_outer') # Could also use 'left_outer'\n",
    "left_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c334be19-6114-4a01-8938-96af8293a97b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|     department|count|\n",
      "+---------------+-----+\n",
      "|      beverages| 4365|\n",
      "|   meat seafood|  907|\n",
      "|           null|    1|\n",
      "|         frozen| 4007|\n",
      "|           deli| 1322|\n",
      "|dry goods pasta| 1858|\n",
      "|           bulk|   38|\n",
      "|          other|  548|\n",
      "|         babies| 1081|\n",
      "|         bakery| 1516|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49688"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_sector = left_join.groupBy(\"department\").count()\n",
    "table_sector.show(10)\n",
    "left_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68b2c4b2-d56a-414c-99ca-047b636fca50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_sector.createOrReplaceTempView(\"left_join2\")\n",
    "display(spark.sql(\"\"\"\n",
    "  select department, count\n",
    "  from left_join2\n",
    "  order by 2 desc\n",
    "  limit 10\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b557315-536b-418b-aace-64572e6702f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|     49688|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_sector.groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0e879b1-a9de-41b7-8bd7-0080bb8f89a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_sector.createOrReplaceTempView(\"left_join2\")\n",
    "display(spark.sql(\"\"\"\n",
    "  select department, count\n",
    "  from left_join2\n",
    "  order by 2 desc\n",
    "  limit 10\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "Carvigiani_DelleFratte_Picconi_Tommasino",
   "notebookOrigID": 71453532229948,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
